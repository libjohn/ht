---
title: "combining IVY+ dataframes"
output: html_notebook
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(stringr)
```

## load data

`read_delim` will imputed data types of columns from the first 1000 rows.  Later steps will transform all dataframes to share the same column types.  Four files had project failures and should be investigated.
```{r load-data}
JHU <- read_delim("~/R/github/ht/data/JHU_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

UPenn <- read_delim("~/R/github/ht/data/UPenn_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

MIT <- read_delim("~/R/github/ht/data/MIT_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Cornell <- read_delim("~/R/github/ht/data/Cornell_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Brown <- read_delim("~/R/github/ht/data/Brown_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Duke <- read_delim("~/R/github/ht/data/Duke_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Yale <- read_delim("~/R/github/ht/data/Yale_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

UChicago <- read_delim("~/R/github/ht/data/UChicago_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Harvard <- read_delim("~/R/github/ht/data/Harvard_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Princeton <- read_delim("~/R/github/ht/data/Princeton_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

Columbia <- read_delim("~/R/github/ht/data/Columbia_Normalized.tsv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)

```

## Idenitfy Standard data structure

We'll use `glimpse()` and the JHU dataframe to identify the standard data types across all variables in all dataframes.
```{r}
glimpse(JHU)
```


## Transform variables

Three files were imputed with incorrect guesses

1. `read_delim()` imputed *UChicago* `gov_doc` to `character`.  Transformed below to `integer` 
2. `read_delim()` imputed *Princeton* & *Columbia* `pub_year` to `character`. Transformed below to `integer`
```{r}
glimpse(UChicago)
UChicago2 <-  UChicago %>% 
  mutate(gov_doc = as.integer(gov_doc))
glimpse(UChicago2)
```

```{r}
glimpse(Princeton)
Princeton2 <- Princeton %>% 
  mutate(pub_year = as.integer(pub_year)) %>% 
  mutate(call_no = callno)
Princeton2$callno <- NULL
glimpse(Princeton2)
```

```{r}
glimpse(Columbia)
Columbia2 <- Columbia %>% 
  mutate(pub_year = as.integer(pub_year)) %>% 
  mutate(call_no = callno)
Columbia2$callno <- NULL
glimpse(Columbia2)
```


## Combine all dataframes into one 
```{r}

big_df <- bind_rows(JHU, UPenn, MIT, 
                    Cornell, Brown, 
                    Yale, Harvard, Duke, 
                    UChicago2,   # as.integer(gov_docs) 
                    Princeton2,  # as.integer(pub_year)
                    Columbia2)   # as.integer(pub_year)

```

## Further Transformations

Per July 6 meeting.  Group decided on the following transformations as part of the data cleaning process...

- Make LC Class all upper case
- Set non-four-digit years to _blank_
- Set all years not between 1440 and 2017 to _blank_
- Compute new column with facet count for resolved_OCLC, i.e., frequency count for each row based on resolved_OCLC

### Make LC Class all uppercase

```{r}
big_df <- big_df %>% 
  mutate(lc_class = str_to_upper(lc_class))
```

### pub_year integrity transformation

- Keep only 4-digit years in pub_year
- Set non--four-digit years to blank
- keep only pub_year between 1440 and 2017

```{r}
big_df <- big_df %>% 
  mutate(pub_year = if_else(str_detect(pub_year, 
                                       "\\d{4}"), 
                            as.character(pub_year), 
                            NA_character_)) %>%
  mutate(pub_year = as.integer(pub_year)) %>% 
  mutate(pub_year = if_else(pub_year >= 1440 
                            & pub_year <= 2017, 
                            pub_year, NA_integer_))
```

###  Frequency of resolved_OCLC Number

Compute new column with facet count for `resolved_OCLC`, i.e., frequency count for each row based on resolved_OCLC


```{r}
big_df %>% 
  count(resolved_oclc) %>% 
  arrange(-n)
```

#### List `member_id`
```{r}
big_df %>% 
  select(member_id) %>% 
  count(member_id)
```

#### Find examples of duplicates for each institution
```{r}
big_df %>% 
  filter(member_id == "yale") %>% # brown, columbia, cornell
  # duke, harvard, JHU, mit, princeton, uchicago, upenn, yale
  count(primary_key) %>% 
  arrange(-n)
```



#### filter on a specific `resolved_oclc`

resolved_oclc == 883016 has 7 rows  
```{r}
big_df %>% 
  filter(resolved_oclc == 883016)  # 42 ; rows 411-420
```

```{r}
big_df %>% 
  filter(resolved_oclc == 493136) %>% 
  group_by(member_id) %>% # columbia & princeton
  select(member_id, call_no, 
         lc_class, primary_key, resolved_oclc)
  
```


#### filter on a specific `resolved_oclc`

resolved_oclc == 276173 has 116 rows
```{r}
big_df %>% 
  filter(resolved_oclc == 276173) # row 10 ; rows 10
```

#### Distinct primary_key

looks like Princeton (perhaps; more research) has 50K duplicate rows by primary_key.  Use `distinct()` to retain only unique/distinct rows from an input tbl.
```{r}
big_distinct_primary_key_df <-  big_df %>% 
  distinct(primary_key, .keep_all = TRUE) 

big_distinct_primary_key_df
```

##### Has Call No ?

```{r}
foo <- big_distinct_primary_key_df %>% 
  select(resolved_oclc, call_no, lc_class) %>% 
  arrange(call_no)

tail(head(foo, 3329285))  # ~3,329,285 have callno #
```

##### Any Call No. in this group?

```{r}
big_df %>% 
  filter(resolved_oclc == 8785177) %>% 
  select(call_no, callno)
```




#### Testing

Just verifying my process of grouping and counting by using a less ambiguous variable:  `member_id`
```{r}
big_df %>% 
  group_by(member_id) %>% 
  count(member_id) %>% 
  arrange(-n)

```

### Tryin without duplicate resolved_oclc

No
- Princeton
- Columbia
- harvard

```{r}
big_no_dupe_df <- big_df %>% 
  filter(member_id != "princeton" & 
           member_id != "columbia" & 
           member_id != "harvard")
```



```{r}
big_no_dupe_df %>% 
  count(resolved_oclc) %>% 
  arrange(-n)
```

```{r}
big_no_dupe_df %>% 
  filter(resolved_oclc == 214718 |
           resolved_oclc == 1467107 |
           resolved_oclc == 137189 |
           resolved_oclc == 2438338 | 
           resolved_oclc == 14705951 | 
           resolved_oclc == 300389) %>% 
  arrange(resolved_oclc)
```



## Write combined dataframe as one TSV file
```{r}
write.table(big_df, file = "data/combined.tsv", sep = "\t",  row.names = FALSE)
```


## Failures

Note that four dataframes had parsing failures.  See console messages integrated into the project notebook ('combine.nb.html') for details.

1. Cornell had 26 parsing failures out of over 1 Million rows.  The failures appear to be in the pub_year variable.
```{r}
problems(Cornell)
```


1. Brown had 43 parsing failures out of 165,000 rows.  The failures appear to be in the pub_year variable.

```{r}
problems(Brown)
```



1. University of Harvard had 489 parsing failures out of almost 670,000 rows.  The failures appear to be in the pub_year variable

```{r}
problems(Harvard)
```



